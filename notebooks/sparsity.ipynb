{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn as sk \n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from jax import vmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_sparsity(params):\n",
    "    return jnp.linalg.norm(params, 1)\n",
    "\n",
    "def l1_sparsity_activation(est_by_term):\n",
    "    # assume that est_by_term is of shape (N, p)\n",
    "    return jnp.linalg.norm(est_by_term, ord = 1, axis= 1)\n",
    "\n",
    "def kld_sparsity_activation(est_by_term):\n",
    "    # est_by_term: shape (N, p)\n",
    "    N, p = est_by_term.shape\n",
    "\n",
    "    # Normalize each row to form a distribution\n",
    "    normed = est_by_term / (jnp.linalg.norm(est_by_term, axis=1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    # Define the target (uniform) distribution\n",
    "    uniform = jnp.ones((p,)) / p\n",
    "\n",
    "    # Compute KL divergence for each row\n",
    "    def row_kld(row):\n",
    "        return jnp.sum(jax.scipy.special.kl_div(row, uniform))\n",
    "\n",
    "    # vmap across rows\n",
    "    kld_values = vmap(row_kld)(jnp.abs(normed))  # shape (N,)\n",
    "    return kld_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(2)\n",
    "X = jax.random.normal(key, (5, 10))\n",
    "kld_sparsity_activation(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ests = jax.random.normal(jax.random.PRNGKey(3), (100, 4))\n",
    "kld_sparsity_activation(ests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import grad, jit, vmap\n",
    "from jax.scipy.special import kl_div\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def generate_data(N=100, p=20, sparsity_level=0.3, seed=0):\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    num_nonzero = int(p * sparsity_level)\n",
    "    beta_true = jnp.zeros(p)\n",
    "    nonzero_indices = jax.random.choice(key, p, (num_nonzero,), replace=False)\n",
    "    beta_true = beta_true.at[nonzero_indices].set(jax.random.normal(key, (num_nonzero,)))\n",
    "    key, subkey = jax.random.split(key)\n",
    "    X = jax.random.normal(subkey, (N, p))\n",
    "    noise = 0.1 * jax.random.normal(subkey, (N,))\n",
    "    y = X @ beta_true + noise\n",
    "    return X, y, beta_true\n",
    "\n",
    "def predict(X, beta):\n",
    "    return X @ beta\n",
    "\n",
    "def mse_loss(X, y, beta):\n",
    "    return jnp.mean((predict(X, beta) - y) ** 2)\n",
    "\n",
    "def l1_param(beta):\n",
    "    return jnp.sum(jnp.abs(beta))\n",
    "\n",
    "def l1_activation(X, beta):\n",
    "    act = X * beta  # shape (N, p)\n",
    "    return jnp.mean(jnp.linalg.norm(act, ord=1, axis=1))\n",
    "\n",
    "def kl_activation(X, beta, eps=1e-8):\n",
    "    act = jnp.abs(X * beta)\n",
    "    normed = act / (jnp.linalg.norm(act, axis=1, keepdims=True) + eps)\n",
    "    uniform = jnp.ones(X.shape[1]) / X.shape[1]\n",
    "    def row_kl(row): return jnp.sum(kl_div(row, uniform))\n",
    "    return jnp.mean(vmap(row_kl)(normed))\n",
    "\n",
    "@partial(jit, static_argnames=[\"penalty_type\"])\n",
    "def loss_fn(beta, X, y, lam, penalty_type):\n",
    "    if penalty_type == \"l1_param\":\n",
    "        return mse_loss(X, y, beta) + lam * l1_param(beta)\n",
    "    elif penalty_type == \"l1_activation\":\n",
    "        return mse_loss(X, y, beta) + lam * l1_activation(X, beta)\n",
    "    elif penalty_type == \"kl_activation\":\n",
    "        return mse_loss(X, y, beta) + lam * kl_activation(X, beta)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown penalty type: {penalty_type}\")\n",
    "\n",
    "def fit_model(X, y, lam, penalty_type, num_steps=1000, lr=1e-2):\n",
    "    beta = jnp.zeros(X.shape[1])\n",
    "    optimizer = optax.sgd(lr)\n",
    "    opt_state = optimizer.init(beta)\n",
    "\n",
    "    def step(beta, opt_state):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(beta, X, y, lam, penalty_type)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        beta = optax.apply_updates(beta, updates)\n",
    "        return beta, opt_state\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        beta, opt_state = step(beta, opt_state)\n",
    "    return beta\n",
    "\n",
    "def compare_sparsity_models(N=100, p=20, sparsity_level=0.3, lam=1.0):\n",
    "    X, y, beta_true = generate_data(N, p, sparsity_level)\n",
    "\n",
    "    beta_l1 = fit_model(X, y, lam, \"l1_param\")\n",
    "    beta_l1act = fit_model(X, y, lam, \"l1_activation\")\n",
    "    beta_klact = fit_model(X, y, lam, \"kl_activation\")\n",
    "\n",
    "    return {\n",
    "        \"beta_true\": beta_true,\n",
    "        \"beta_l1\": beta_l1,\n",
    "        \"beta_l1_activation\": beta_l1act,\n",
    "        \"beta_kl_activation\": beta_klact\n",
    "    }\n",
    "\n",
    "def plot_coefficients(results):\n",
    "    p = results[\"beta_true\"].shape[0]\n",
    "    labels = [f\"$\\\\beta_{{{i}}}$\" for i in range(p)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    width = 0.2\n",
    "    x = jnp.arange(p)\n",
    "\n",
    "    ax.bar(x - 1.5*width, results[\"beta_true\"], width=width, label=\"True\", color=\"black\")\n",
    "    ax.bar(x - 0.5*width, results[\"beta_l1\"], width=width, label=\"L1 Weights\", alpha=0.7)\n",
    "    ax.bar(x + 0.5*width, results[\"beta_l1_activation\"], width=width, label=\"L1 Activation\", alpha=0.7)\n",
    "    ax.bar(x + 1.5*width, results[\"beta_kl_activation\"], width=width, label=\"KL Activation\", alpha=0.7)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45)\n",
    "    ax.set_ylabel(\"Coefficient Value\")\n",
    "    ax.set_title(\"Comparison of Learned Coefficients with Different Sparsity Penalties\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run and plot\n",
    "results = compare_sparsity_models()\n",
    "plot_coefficients(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_sparsity_models()\n",
    "\n",
    "for name, beta in results.items():\n",
    "    print(f\"{name}: {beta.round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_sample():\n",
    "    x = np.random.uniform(-1, 1, 2)\n",
    "    y = np.random.uniform(-1, 1, 2)\n",
    "    return np.kron(x, y)\n",
    "\n",
    "A = np.vstack([generate_sample() for _ in range(4)])\n",
    "rank = np.linalg.matrix_rank(A)\n",
    "print(\"Rank of A:\", rank)\n",
    "\n",
    "for _ in range(500):\n",
    "    A = np.vstack([generate_sample() for _ in range(4)])\n",
    "    rank = np.linalg.matrix_rank(A)\n",
    "    if rank != 4:\n",
    "        print('rank is not 4')\n",
    "        print(rank) \n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sde_inf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
