{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_complex_mult_data(num_points):\n",
    "    x = jax.random.uniform(jax.random.PRNGKey(0), (num_points,2), minval=-1, maxval=1)\n",
    "    y = jax.random.uniform(jax.random.PRNGKey(1), (num_points,2), minval=-1, maxval=1)\n",
    "    # return x, y and the product of x and y\n",
    "    prod_real = x[:,0] * y[:,0] - x[:,1] * y[:,1]\n",
    "    prod_imag = x[:,0] * y[:,1] + x[:,1] * y[:,0]\n",
    "    # stack the real and imaginary parts to get the complex product\n",
    "    z = jnp.stack((prod_real, prod_imag), axis=1)\n",
    "    return x, y, z\n",
    "\n",
    "def expand_dim_complex_dataset(expansion_dim, x, y, z):\n",
    "    projection_matrix = jax.random.normal(jax.random.PRNGKey(0), (expansion_dim, 2))\n",
    "    x_expanded = x @ projection_matrix.T\n",
    "    y_expanded = y @ projection_matrix.T\n",
    "    z_expanded = z @ projection_matrix.T\n",
    "    return x_expanded, y_expanded, z_expanded, projection_matrix\n",
    "\n",
    "#\n",
    "# \n",
    "# def expand_dim_complex_dataset_poly(expansion_dim, x, y, z, max_degree = 4):\n",
    "\n",
    "def init_structure_constants(num_basis_fxn):\n",
    "    # Initialize the structure constants\n",
    "    structure_constants = jax.random.normal(jax.random.PRNGKey(0), (num_basis_fxn, num_basis_fxn, num_basis_fxn))\n",
    "    return structure_constants\n",
    "\n",
    "# def multiply_with_structure_consts(x, y, structure_constants):\n",
    "#     # Perform the multiplication with the structure constants\n",
    "#     z = jnp.einsum('i,j,kij->k', x, y, structure_constants) \n",
    "#     return z\n",
    "\n",
    "# Corrected multiplication function\n",
    "def multiply_with_structure_consts(x, y, structure_constants):\n",
    "    # first check if its batch or 1d \n",
    "    if x.ndim == 1:\n",
    "        x = x[None, :]\n",
    "    if y.ndim == 1:\n",
    "        y = y[None, :]\n",
    "    return jnp.einsum('kij,bi,bj->bk', structure_constants, x, y)\n",
    "\n",
    "# Data generation remains correct\n",
    "def create_random_complex_mult_data(num_points):\n",
    "    x = jax.random.uniform(jax.random.PRNGKey(0), (num_points,2), minval=-1, maxval=1)\n",
    "    y = jax.random.uniform(jax.random.PRNGKey(1), (num_points,2), minval=-1, maxval=1)\n",
    "    prod_real = x[:,0] * y[:,0] - x[:,1] * y[:,1]\n",
    "    prod_imag = x[:,0] * y[:,1] + x[:,1] * y[:,0]\n",
    "    z = jnp.stack((prod_real, prod_imag), axis=1)\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# def pred_with_dim_red(x, y, dim_reducer, struct_consts):\n",
    "#     # Initialize the structure constants\n",
    "#     # Perform the multiplication with the structure constants\n",
    "#     x_hat = dim_reducer(x)\n",
    "#     y_hat = dim_reducer(y)\n",
    "#     z = multiply_with_structure_consts(x, y, struct_consts)\n",
    "#     return z\n",
    "\n",
    "x, y, z = create_random_complex_mult_data(3)\n",
    "x_expanded, y_expanded, z_expanded, projection_matrix = expand_dim_complex_dataset(100, x, y, z)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import value_and_grad\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "def test_complex_right_vector_struct():\n",
    "    # Generate data\n",
    "    x, y, z_true = create_random_complex_mult_data(1000)\n",
    "    \n",
    "    # Initialize learnable parameters\n",
    "    structure_constants = init_structure_constants(2)\n",
    "    \n",
    "    # Loss function\n",
    "    def loss(params, x, y, z_true):\n",
    "        z_pred = multiply_with_structure_consts(x, y, params)\n",
    "        return jnp.mean((z_pred - z_true)**2)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optax.adam(1e-3)\n",
    "    opt_state = optimizer.init(structure_constants)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(5000):\n",
    "        l, grads = value_and_grad(loss)(structure_constants, x, y, z_true)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        structure_constants = optax.apply_updates(structure_constants, updates)\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {l:.4f}\")\n",
    "\n",
    "    # Compare learned vs true constants\n",
    "    true_C = jnp.array([\n",
    "        [[1., 0.], [0., -1.]],  # Real part coefficients\n",
    "        [[0., 1.], [1., 0.]]     # Imaginary part coefficients\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nLearned structure constants:\")\n",
    "    print(structure_constants)\n",
    "    print(\"\\nTrue structure constants:\")\n",
    "    print(true_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "def test_complex_high_dim_vector_struct():\n",
    "    # Generate data\n",
    "    x, y, z_true = create_random_complex_mult_data(1000)\n",
    "    x_expanded, y_expanded, z_expanded, projection_matrix = expand_dim_complex_dataset(100, x, y, z_true)\n",
    "    \n",
    "    # Initialize learnable parameters\n",
    "    structure_constants = init_structure_constants(2)\n",
    "    \n",
    "    # Loss function\n",
    "    def loss(params, x, y, z_true):\n",
    "        z_pred = multiply_with_structure_consts(x, y, params)\n",
    "        return jnp.mean((z_pred - z_true)**2)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optax.adam(1e-3)\n",
    "    opt_state = optimizer.init(structure_constants)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(5000):\n",
    "        l, grads = value_and_grad(loss)(structure_constants, x, y, z_true)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        structure_constants = optax.apply_updates(structure_constants, updates)\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {l:.4f}\")\n",
    "\n",
    "    # Compare learned vs true constants\n",
    "    true_C = jnp.array([\n",
    "        [[1., 0.], [0., -1.]],  # Real part coefficients\n",
    "        [[0., 1.], [1., 0.]]     # Imaginary part coefficients\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nLearned structure constants:\")\n",
    "    print(structure_constants)\n",
    "    print(\"\\nTrue structure constants:\")\n",
    "    print(true_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_C = jnp.array([\n",
    "    [[1., 0.], [0., -1.]],  # Real part coefficients\n",
    "    [[0., 1.], [1., 0.]]     # Imaginary part coefficients\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_complex_right_vector_struct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from functools import partial \n",
    "\n",
    "class ComplexMultiplicationModel(nn.Module):\n",
    "    embed_dim: int = 2\n",
    "    mlp_hidden: int = 64\n",
    "    use_mlp: bool = True\n",
    "    expansion_dim: int = 100\n",
    "    \n",
    "    def setup(self):\n",
    "        # Learnable encoder/decoder projections\n",
    "        if self.use_mlp:\n",
    "            self.encoder = nn.Sequential([\n",
    "                nn.Dense(self.mlp_hidden),\n",
    "                nn.relu,\n",
    "                nn.Dense(self.embed_dim)\n",
    "            ])\n",
    "            self.decoder = nn.Sequential([\n",
    "                nn.Dense(self.mlp_hidden),\n",
    "                nn.relu,\n",
    "                nn.Dense(self.expansion_dim)\n",
    "            ])\n",
    "        else:\n",
    "            self.encoder = nn.Dense(self.embed_dim)\n",
    "            self.decoder = nn.Dense(self.expansion_dim)\n",
    "            \n",
    "        # Learnable structure constants\n",
    "        self.structure_constants = self.param('C', \n",
    "            nn.initializers.normal(0.1), \n",
    "            (self.embed_dim, self.embed_dim, self.embed_dim))\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        # Encode inputs to latent space\n",
    "        x_embed = self.encoder(x)\n",
    "        y_embed = self.encoder(y)\n",
    "        \n",
    "        # Complex multiplication in latent space\n",
    "        product = multiply_with_structure_consts(x_embed, y_embed, self.structure_constants)\n",
    "        #product = jnp.einsum('bi,bj,ijk->bk', x_embed, y_embed, self.structure_constants)\n",
    "        z_projected = self.decoder(product)\n",
    "        x_reconstructed = self.decoder(x_embed)\n",
    "        y_reconstructed = self.decoder(y_embed)\n",
    "        \n",
    "        # Decode back to expanded space\n",
    "        return z_projected, x_reconstructed, y_reconstructed\n",
    "\n",
    "\n",
    "def structure_const_entropy_loss(matrix):\n",
    "    \"\"\"\n",
    "    The structure const is an NxNxN tensor, where N is the number of basis functions.\n",
    "    For each structure const, structure_const[i, :, :], we want to ensure that the rows and \n",
    "    columns look like one hot vectors. So we define a loss as H(rows) + H(columns), where H is the entropy.\n",
    "    \"\"\"\n",
    "    entropy = lambda x: jnp.sum(x * jnp.log(x + 1e-10), axis=-1)\n",
    "    row_entropy = entropy(jnp.abs(matrix))\n",
    "    col_entropy = entropy(jnp.abs(matrix.transpose(0, 2, 1)))\n",
    "    return jnp.mean(row_entropy + col_entropy)\n",
    "\n",
    "def train_model(x_e, y_e, z_e, config):\n",
    "    model = ComplexMultiplicationModel(**config)\n",
    "    \n",
    "    def loss(params, x, y, z):\n",
    "        pred, x_recon, y_recon = model.apply(params, x, y)\n",
    "        mse_z = jnp.mean((pred - z) ** 2)\n",
    "        mse_x = jnp.mean((x_recon - x) ** 2)\n",
    "        mse_y = jnp.mean((y_recon - y) ** 2)\n",
    "        C = params['params']['C']\n",
    "        entropy_lambda = 0\n",
    "        entropy_loss = structure_const_entropy_loss(C)\n",
    "        return mse_z + mse_x + mse_y + entropy_lambda * entropy_loss\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss)\n",
    "\n",
    "    # Initialize\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    params = model.init(rng, x_e[0], y_e[0])\n",
    "    optimizer = optax.adamw(1e-3, weight_decay=1e-4)\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    @jax.jit\n",
    "    def update(params, opt_state, x, y, z):\n",
    "        l, grads = grad_fn(params, x, y, z)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params)  # Pass params here\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, l\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(5000):\n",
    "        params, opt_state, current_loss = update(params, opt_state, x_e, y_e, z_e)\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {current_loss:.4f}\")\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_const_entropy_loss(jnp.array([[[5, 5], [2, -1.]], [[2., 1.], [1., 0.]]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate expanded data\n",
    "x, y, z = create_random_complex_mult_data(1000)\n",
    "x_e, y_e, z_e, proj_mat = expand_dim_complex_dataset(100, x, y, z)\n",
    "\n",
    "# Train with different configurations\n",
    "config = {\n",
    "    'embed_dim': 2,\n",
    "    'mlp_hidden': 64,\n",
    "    'use_mlp': False,\n",
    "    'expansion_dim': 100\n",
    "}\n",
    "\n",
    "params = train_model(x_e, y_e, z_e, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_const_entropy_loss(params['params']['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, inspect the learned structure constants\n",
    "learned_constants = params['params']['C']  # Access the 'C' parameter we defined\n",
    "\n",
    "print(\"Learned Structure Constants:\")\n",
    "print(learned_constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structure_constants(basis_elts, proj_func, multiplication_function) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Given a list of basis elements, return the structure constants.\n",
    "    \"\"\"\n",
    "    num_basis = len(basis_elts)\n",
    "    structure_constants = jnp.zeros((num_basis, num_basis, num_basis))\n",
    "    \n",
    "    for gamma, target_basis in enumerate(basis_elts):\n",
    "        for alpha, a in enumerate(basis_elts):\n",
    "            for beta, b in enumerate(basis_elts):\n",
    "                # Get the projector for the target basis\n",
    "                #proj_func = basis_element_projectors[gamma]\n",
    "                # Compute the structure constant\n",
    "                projection_val = proj_func(multiplication_function(a,b), target_basis)\n",
    "                structure_constants = structure_constants.at[gamma, alpha, beta].set(projection_val)\n",
    "    \n",
    "    return structure_constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_proj_func(z, target_basis):\n",
    "    return jnp.dot(jnp.array([z.real, z.imag]), jnp.array([target_basis.real, target_basis.imag]))\n",
    "\n",
    "def complex_multiplication_function(z1, z2):\n",
    "    return z1 * z2\n",
    "\n",
    "basis_elts = [1, 1j]\n",
    "structure_constants = get_structure_constants(basis_elts, complex_proj_func, complex_multiplication_function)\n",
    "print(structure_constants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basis_elts = [ (1 - 1j)/jnp.sqrt(2), (1 + 1j)/jnp.sqrt(2)]\n",
    "structure_constants = get_structure_constants(basis_elts, complex_proj_func, complex_multiplication_function)\n",
    "print(structure_constants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = jnp.array([[1, 1],[1, -1]]) * (1/jnp.sqrt(2))\n",
    "P_inv = jnp.linalg.inv(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_mat = jnp.array([[1, 0], [0, 1]])\n",
    "i_mat = jnp.array([[0, -1], [1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_proj_func_mat(z, target_basis):\n",
    "    return 1/2 * jnp.trace(target_basis.T @ z)\n",
    "\n",
    "def complex_multiplication_function_mat(z1, z2):\n",
    "    return z1 @ z2\n",
    "\n",
    "basis_elts = [one_mat, i_mat]\n",
    "structure_constants = get_structure_constants(basis_elts, complex_proj_func_mat, complex_multiplication_function_mat)\n",
    "print(structure_constants)\n",
    "\n",
    "def two_d_rotation_matrix(theta):\n",
    "    return jnp.array([[jnp.cos(theta), -jnp.sin(theta)], [jnp.sin(theta), jnp.cos(theta)]])\n",
    "theta = jnp.pi/4\n",
    "R = two_d_rotation_matrix(theta)\n",
    "R_inv = two_d_rotation_matrix(-theta)\n",
    "\n",
    "R_inv @ i_mat @ R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sde_inf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
